I"òK<p>After spending some time learning more about causality in more detail, I decided to post some thoughts about the implications of <strong>confounding</strong> and <strong>collider bias</strong> in causal modelling.</p>

<p align="center">
    <img src="/assets/img/post_13/matrix_pill.jpg" /> 
</p>

<h1 id="simple-causality-primer">Simple Causality Primer</h1>
<p>The idea behind Causality is to identify the treatment effect (typically named as T) on an outcome response (typically named as Y). For example, if you had a headache, you would want to find out if taking a particular pill (T) would cure the headache (Y).</p>

<p align="center">
    <img src="/assets/img/post_13/headache_example.png" /> 
</p>

<p>The fundamental problem of causal inference stems from the fact that you can only observe one outcome for a given single ‚Äúunit‚Äù. A ‚Äúunit‚Äù is used to refer to a specific experimental unit/iteration, which in this case refers to your particular experience of having the headache at that point in time. You can only choose to either:</p>
<ol>
  <li>Take the pill [T = 1] or</li>
  <li>Not to take the pill [T = 0].</li>
</ol>

<p>Consequently, you get to observe the outcome from your chosen treatment (<code class="highlighter-rouge">Y_t | T = t</code>, where t represents 1 or 0), but not the outcome of the treatment that you did not choose to take. The latter is known as the <strong>counterfactual</strong>, which represents the outcome of ‚Äúwhat would have happened‚Äù. To tackle this fundamental problem, one would have to refer to the potential outcomes causal model framework by Rubin, whereby you need to make assumptions to estimate the missing counterfacual.</p>

<p>In the example above, suppose you decide to take the pill. Thus, you would have observed the outcome for <code class="highlighter-rouge">Y_1 | T = 1</code>, but you would not have observed the counterfactual outcome for not taking the pill <code class="highlighter-rouge">Y_0 | T = 0</code>. At an individual level, it will be impossible to obtain the counterfactual outcome, and thus it is impossible to calculate the individual treatment effect as measured by the difference in observed outcome versus counterfactual outcome.</p>

<p>By conducting experiments at a population level, we can calculate the <strong>average treatment effect</strong>. As shown by in the following table, we have 8 participants where 4 of them having T = 1 vs 4 of them having T = 0. The counterfactual outcome for each individual is represented by the question mark.</p>

<p align="center">
    <img src="/assets/img/post_13/experiment_table_1.png" /> 
</p>

<p>What is intuitive or commonsensical is that in the above example, we can calculate the average treatment effect as <code class="highlighter-rouge">E(Y_1 - Y_0)</code> = 3/4 - 2/4 = 0.25. To arrive at that conclusion, we made some assumptions to identify the causal effects such that it can be mathematically calculated as a difference between outcomes of the different treatment groups.</p>

<h1 id="assumptions-behind-identifiability-of-causal-effects">Assumptions behind Identifiability of Causal Effects</h1>

<p>The identifiability of causal effects requires making some untestable assumptions called <strong>causal assumptions</strong>:</p>

<h2 id="stable-unit-treatment-value-assumption-sutva">Stable Unit Treatment Value Assumption (SUTVA)</h2>

<p>As a concept, SUTVA involves two implicit assumptions:</p>
<ol>
  <li>Units do not interfere with each other. In other words, the treatment assignment of one unit does not affect the OUTCOME of another unit.</li>
  <li>There is onnly one version of treatment that is consistent across all units.</li>
</ol>

<h2 id="consistency">Consistency</h2>

<p>The consistency assumption states that the potential outcome under treatment T = t, <code class="highlighter-rouge">Y^t</code> is equal to the observed outcome if the actual treatment received is T = t.</p>

<h2 id="positivity">Positivity</h2>

<p>The positivity assumption states that for every set of values for X covariates, treatment assignment was not deterministic.</p>

<p align="center">
    <img src="/assets/img/post_13/eqn_positivity_1.png" /> 
</p>

<p>If this was violated, it will mean that for a given value of X, everybody is either treated or not treated at all. Within that level of X, it will be impossible to learn the causal treatment effect.</p>

<p>Suppose that for some values of X, the treatment was deterministic as represented by the following example where there is no control group:</p>

<p align="center">
    <img src="/assets/img/post_13/eqn_positivity_2.png" /> 
</p>

<p>then we would have no observed values of Y for the control group for those values of X.</p>

<h2 id="ignorability">Ignorability</h2>

<p>This is also known as the no unmeasured confounders assumption. In mathematical form, the potential outcomes are independent from the treatment assignment.</p>

<p align="center">
    <img src="/assets/img/post_13/eqn_ignorability_1.png" /> 
</p>

<p>Under situations where you have additional covariates commonly represented by X, it represents the <strong>conditional independence of treatment assignment from potential outcomes</strong>.</p>

<p align="center">
    <img src="/assets/img/post_13/eqn_ignorability_2.png" /> 
</p>

<p><strong>Ignorability</strong> is the key assumption that allows us to use the outcomes from different units under different treatment as the corresponding counterfactuals outcomes, and thus ‚Äúpopulate‚Äù the ‚Äú?‚Äù marks that we saw in a previous example. The concept stems from the idea that given the right covariates, treatment itself becomes ignorable.</p>

<p>Revisiting the previous example of an experiment conducted on 8 participants, we also expanded the table by including the Gender as a covariate X.</p>

<p align="center">
    <img src="/assets/img/post_13/experiment_table_2.png" /> 
</p>

<p>In this example, we have great balance across different values of X. Both genders (Male or Female) have 4 experimental units each, with 2 the in treatment group and 2 in the control group.</p>

<p>With the presence of the covariate X, we can determine the <strong>conditional average treatment effect</strong> (CATE) which is also known as <strong>heterogenous treatment effect</strong>. With respect to the example, we have the following calculations:</p>

<p>CATE for Males = <code class="highlighter-rouge">E(Y1 - Y2 | X = M)</code> = (1/2) - (1/2) = 0.5 - 0.5 = 0</p>

<p>CATE for Females = <code class="highlighter-rouge">E(Y1 - Y2 | X = F)</code> = (2/2) - (1/2) = 1 - 0.5 = 0.5</p>

<h1 id="confounding-in-causality">Confounding in Causality</h1>

<p>To understand how confounding comes into play within causality, we can take a look at a system that has 3 variables, where X represents the confounder. This cna be represented by the graphical model shown below, where the nodes represent the variables and the directed edges represent the relationship with a causal direction.</p>

<p align="center">
    <img src="/assets/img/post_13/dag_confounder_1.png" /> 
</p>

<p>Note that this graph is a representation of a directed acyclic graph (DAG) where:</p>
<ol>
  <li>All the edges between nodes have direction</li>
  <li>There is no cyclicity in the causal relationship between nodes.</li>
</ol>

<p>As shown, the confounder affects both the treatment variable T and response variable Y. An example of this could be X as Age, T as Physical Activity, Y as Mortality. In this hypothetical example, when one is younger, you tend to have greater physical activity and at the same time, your immune system is stronger. As you get older, you tend to have less physical activity and at the same time, your mortality increases perhaps due to a weakening immune system.</p>

<p>Thus, the direct causal relationship between physical activity as a treatment variable on the outcome variable mortality may not be easily identifiable since there is the confounding variable that affects both the treatment variable and outcome.</p>

<p align="center">
    <img src="/assets/img/post_13/dag_confounder_2.png" /> 
</p>

<p>When we speak about causality, we want to identify the direct causal association between the treatment variable on the outcome variable (as shown by the red arrow). In the context of confounding variables, there is always indirect association that runs through the confounder variable (as shown by the blue arrow). Note that this is a simplified graph, but this can be extended to other graphs where X itself is a series of nodes that represent <strong>forks</strong> or <strong>chains</strong>.</p>

<p align="center">
    <img src="/assets/img/post_13/dag_confounder_3.png" /> 
</p>

<h1 id="independence-and-conditional-independence-of-variables">Independence and Conditional Independence of Variables</h1>

<p>When variables are independent of each other, that means that knowing one variable does not change the probability of determining the other. In other words, for two independent variables A and B,</p>

<p align="center">
    <img src="/assets/img/post_13/eqn_independence_1.png" /> 
</p>
<p align="center">
    <img src="/assets/img/post_13/eqn_independence_2.png" /> 
</p>

<p>Conditional independence between two variables A and B involve extra variables to be conditioned on. For two variables A and B that are conditionally independent on variable C,</p>

<p align="center">
    <img src="/assets/img/post_13/eqn_cond_independence_1.png" /> 
</p>
<p align="center">
    <img src="/assets/img/post_13/eqn_cond_independence_2.png" /> 
</p>

<p>For graphical relationships represented by Forks and Chains, we observe:</p>
<ul>
  <li><strong>Conditional independence</strong> between the ‚Äúside‚Äù nodes if we condition upon the ‚Äúcenter‚Äù variable (shown by the shading). This is also known as ‚Äúblocking‚Äù the dependence association between the ‚Äúside‚Äù nodes.</li>
  <li>Both ‚Äúside‚Äù variables are not independent by themselves.</li>
</ul>

<p>In a <strong>Fork</strong> that stems from X and branches out to T and Y (T &lt;- X -&gt; Y), we observe the following:</p>

<p align="center">
    <img src="/assets/img/post_13/fork_cond_independence.png" /> 
</p>

<p>An example of a Fork situation can be exemplified by rainy weather (represented by X in the graph above) versus the number of vehicle accidents and number of cold flu cases (represented by T and Y).</p>
<ul>
  <li>By themselves, Y and T are dependent on each other. When there is heavy rain, we may observe higher counts of vehicle accidents and cold flu cases, and vice versa.</li>
  <li>By conditioning on rainy weather X, knowing the count of vehicles accidents T does not help you to gain more information about the number of cold flue cases Y. In other words, <code class="highlighter-rouge">P(Y|T,X) = P(Y|X)</code></li>
</ul>

<p>In a <strong>Chain</strong> that stems from T to X to Y (T -&gt; X -&gt; Y), we observe the following:</p>

<p align="center">
    <img src="/assets/img/post_13/chain_cond_independence.png" /> 
</p>

<p>An example of a Chain situation can be exemplified by a genetic disease along an ancestry line where in this case (T -&gt; X -&gt; Y), T is the grandparent, X is the parent, and Y is the child.</p>
<ul>
  <li>By themselves, Y and T will demonstrate dependence between each other. If T has the disease, it is also likely that Y has the disease. If T does not have the disease, it is also likely that Y does not have the disease.</li>
  <li>By conditioning on the middle node X (or knowing what X has), knowing if T has the disease does not help you to gain more information about Y having the disease. In other words, <code class="highlighter-rouge">P(Y|T,X) = P(Y|X)</code></li>
</ul>

<p>For a graphical relationships represented by Colliders, we observe:</p>
<ul>
  <li><strong>Independence</strong> between the ‚Äúside‚Äù nodes <strong>WITHOUT</strong> conditioning upon the ‚Äúcenter‚Äù/‚Äùcollider‚Äù variable.</li>
  <li>Conditioning upon the ‚Äúcollider‚Äù variable (shown by the shading) will open up a dependence association between the ‚Äúside‚Äù variables</li>
</ul>

<p>In a <strong>Collider</strong> that collides at X from T and Y (T -&gt; X &lt;- Y), we observe the following:</p>

<p align="center">
    <img src="/assets/img/post_13/collider_independence.png" /> 
</p>

<p>An example of a Collider situation is the eye colour of parents (represented by T and Y in the graph above) versus the eye colour of their child (represented by X).  Suppose that in this hypothetical world, having green eye colour is a dominant trait.</p>
<ul>
  <li>The eye colour of the parents (T and Y) by themselves are independent of each other.</li>
  <li>By conditioning on the eye colour of the child (X), the eye colour of both parents (T and Y) become conditionally dependent.
    <ul>
      <li>If the child has green eye colour and one parent does not have green eye colour, we can conclude that the other parent must have green eye colour.</li>
      <li>If the child does not have green eye colour and one parent does not have green eye colour, we can conclude that the other parent must not have green eye colour.</li>
    </ul>
  </li>
</ul>

<h1 id="interventional-versus-observational-distributions">Interventional Versus Observational Distributions</h1>

<p>To identify the causal effect of treatment, we need to understand the idea of interventional distributions. Intervention of the treatment variable T implies taking the whole population and subjecting them to the treatment effect. This can be represented by the ‚Äúdo‚Äù operator as shown by the following equation, where Y represents the potential outcome:</p>

<p align="center">
    <img src="/assets/img/post_13/eqn_intervention_1.png" /> 
</p>

<p>Based on this framework, the average treatment effect (ATE) is represented by the following <strong>causal estimand</strong>.</p>

<p align="center">
    <img src="/assets/img/post_13/eqn_intervention_2.png" /> 
</p>

<p>Note that the idea of interventional distributions is not the same as observational distributions (which do not have the ‚Äúdo‚Äù operator). In observational distributions, we deal with ‚Äúconditioning‚Äù that splits the population into subpopulations while in interventional distributions, we deal with ‚Äúdo‚Äù interventions that subject the whole population. This is shown in the diagram as taken from <a href="https://www.bradyneal.com/causal-inference-course">‚ÄúIntroduction to Causal Inference‚Äù</a> textbook by Brady Neal.</p>

<p align="center">
    <img src="/assets/img/post_13/intervention_vs_conditioning_1.png" /> 
</p>

<p>The key difference between interventional distribution and observational distributions is that the covariate distribution between the different subpopulations may not be the same. Recall that we are interested in the <strong>average treatment effect</strong> among the whole population.</p>
<ul>
  <li>In interventional distributions, we are comparing the difference in outcomes between populations that have similar attributes/covariates, and thus it is a fair comparison of treatment effects.</li>
  <li>In observational distributions, this may not necessarily be the same since there might be a confounder that distorts the Treatment assignment among the subpopulations. For example, perhaps Gender (as a confounder) may affect the treatment assignment such that there is a higher proportion of males in the Treated subpopulation while there is a higher proportion of females in Control subpopulation. Thus, we cannot simply find the difference in response outcomes between the subpopulations since it is technically comparing apples and oranges.</li>
</ul>

<p align="center">
    <img src="/assets/img/post_13/intervention_vs_conditioning_2.png" /> 
</p>

<p>Also, interventional distributions are used for <strong>causal estimands</strong> while observational distributions are used for <strong>statistical estimands</strong>. Thus, we need to incorporate causal modelling such that we can identify (or ‚Äúreduce‚Äù) these causal estimands (with ‚Äúdo‚Äù operators) into statistical estimands that are estimatable via available observational data (that do not have ‚Äúdo‚Äù operators).</p>

<h1 id="causal-estimand-identification-with-observational-data-under-various-causal-models">Causal Estimand Identification with Observational Data under various Causal Models</h1>

<h2 id="confounding-variable-context">Confounding variable context</h2>

<p>In the simple example where X is a confounding variable that affects T and Y, there is <strong>direct causal association</strong> flowing from T to Y and <strong>indirect non-causal association</strong> flowing from T to X to Y (as represented by the fork graphical representation which implies T and Y are not independent).</p>

<p align="center">
    <img src="/assets/img/post_13/breakdown_confounding.png" /> 
</p>

<p>Since we are only interested in the direct causal association, this relates back to the idea of interventional distributions. In interventional distributions, we seek to set the treatment variable as a specific value in order to observe the treatment effect on the outcome variable. This implies that <strong>all edges that are incoming into T are eliminated</strong>.</p>

<p align="center">
    <img src="/assets/img/post_13/breakdown_interventional.png" /> 
</p>

<p>This is only achievable either through:</p>
<ol>
  <li>Experiments where the treatment assignment is randomised (thus breaking down the confounding relationship between X and T)</li>
  <li>Observational data where there is no indirect non-causal association flowing.</li>
</ol>

<p>In the second option, with observational data, we can condition on X which breaks the non-causal association between T and Y. This effectively reduces the diagram to one that only has the direct causal association flowing between T and Y. This is also known as the <strong>backdoor adjustment criteria</strong> but I won‚Äôt go into greater detail as it requires much more indepth explanations.</p>

<p align="center">
    <img src="/assets/img/post_13/breakdown_observational.png" /> 
</p>

<p>As shown, this observational distribution (without any ‚Äúdo‚Äù operators) is the equivalent of the interventional distribution (with ‚Äúdo‚Äù operators).</p>

<p>Thus, it is through this identification process that the <strong>causal estimand of the ATE can be identified through a statistical estimand</strong>, which can subsequently be estimated through observational data.</p>

<h2 id="collider-variable-context">Collider variable context</h2>

<p>If the causal model relationship has X that is a collider variable between T and Y, we have to be careful in terms of how to use observational data to identify the causal estimand.</p>

<h1 id="summary">Summary</h1>

<p>In this blog post, I have provided a summarised walkthrough about some introductory concepts about causal inference (including confounding, graphical representations, etc) and how to identify the average treatment effect through observational data with a simplified example.</p>

<p>In the next blog post, I will work through some R code and simulation examples to illustrate how we can use observational data to identify causal effects under the appropriate causal model relationships.</p>

:ET